{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-20T10:49:54.946066Z",
     "start_time": "2017-05-20T16:19:54.451501+05:30"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import sign\n",
    "from numpy import zeros\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from pandas import rolling_median\n",
    "from datetime import timedelta\n",
    "import datetime as dt\n",
    "engine = create_engine('mysql://root:power@2012@localhost/power', echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-20T10:49:55.801781Z",
     "start_time": "2017-05-20T16:19:55.794777+05:30"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    km = 6367 * c\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-20T10:50:55.370189Z",
     "start_time": "2017-05-20T16:19:57.045694+05:30"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weather_data = pd.read_sql_query('''SELECT b.* , a.latitude, a.longitude\n",
    "FROM \n",
    "power.imdaws_wunderground_map a,\n",
    "power.unified_weather b\n",
    "where b.location = a.mapped_location_name \n",
    "and a.discom = \"GUVNL\"''', engine, index_col = None)\n",
    "weather_data = weather_data.loc[weather_data['data_source'] =='IBMWEATHERCHANNEL']\n",
    "weather_data = weather_data.loc[weather_data['data_type'].isin(['FORECAST','ACTUAL'])]\n",
    "weather_data['date'] = pd.to_datetime(weather_data['date'])\n",
    "weather_data.rename(columns={'block_hour_no': 'hour', 'temperature': 'temp',\n",
    "                   'rainfall_mm': 'RainMM',  }, inplace=True)\n",
    "weather_data['year'] = pd.DatetimeIndex(weather_data['date']).year\n",
    "weather_data['month'] = pd.DatetimeIndex(weather_data['date']).month   # jan = 1, dec = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-20T10:50:55.374259Z",
     "start_time": "2017-05-20T16:20:55.371914+05:30"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test1 = weather_data[weather_data['data_type']=='FORECAST']\n",
    "# test2 = weather_data[weather_data['data_type']=='ACTUAL']\n",
    "# len(test1)/(24*20),len(test2)/(24*20),weather_data['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-20T10:51:00.057619Z",
     "start_time": "2017-05-20T16:20:57.920737+05:30"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Missing Data Analysis\n",
    "\n",
    "uniquen_location = weather_data['location'].unique()\n",
    "missing_data = pd.DataFrame([])\n",
    "for i in xrange(0,len(uniquen_location)):\n",
    "    test = weather_data[weather_data['location']==uniquen_location[i]]\n",
    "    test1 = pd.DataFrame(test.isnull().sum())\n",
    "    test1.rename( columns = {0:'missing_count'}, inplace = True)\n",
    "    test1['location']=uniquen_location[i]\n",
    "    test1['count']=len(test)\n",
    "    missing_data = missing_data.append(test1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-20T10:51:01.032742Z",
     "start_time": "2017-05-20T16:21:00.787578+05:30"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "missing_count = weather_data[['temp','windspeed']].\\\n",
    "                groupby(weather_data['location'], as_index = False).\\\n",
    "                agg(['count','size']).reset_index()\n",
    "missing_count.columns = [''.join(col).strip() for col in missing_count.columns.values]\n",
    "missing_count['max_count'] = np.max(missing_count['tempsize'])\n",
    "missing_count['%ge_missing_temp'] = missing_count['tempcount']/missing_count['max_count']\n",
    "missing_count['%ge_windspeedcount'] = missing_count['windspeedcount']/missing_count['max_count']\n",
    "missing_count = missing_count[missing_count['%ge_missing_temp'] > 0.5]\n",
    "missing_count.sort_values(by = ['%ge_missing_temp'], ascending=[True], inplace=True)\n",
    "missing_temp_order = missing_count.location.unique()\n",
    "missing_count.sort_values(by = ['%ge_windspeedcount'], ascending=[True], inplace=True)\n",
    "missing_windspeed_order = missing_count.location.unique()\n",
    "unique_location = np.asarray(missing_count['location'].unique())\n",
    "list(unique_location)\n",
    "lat_long = weather_data[['location','latitude','longitude']]\n",
    "lat_long = lat_long.drop_duplicates()\n",
    "lat_long = lat_long[lat_long['location'].isin(list(unique_location))]\n",
    "\n",
    "weather_data = weather_data[weather_data['location'].isin(list(unique_location))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-20T10:51:02.167901Z",
     "start_time": "2017-05-20T16:21:01.904177+05:30"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def impute(weather_data):\n",
    "    weather_data.sort_values(by = ['location','date','hour'],\n",
    "                         ascending = [True, True, True],\n",
    "                         inplace = True)\n",
    "    weather_data.sort_values(by = ['location','date','hour'], ascending = [True,True,True], inplace = True)\n",
    "    weather_data['lag1_temp'] = weather_data.groupby(['location'])['temp'].shift(1)\n",
    "    weather_data['lead1_temp'] = weather_data.groupby(['location'])['temp'].shift(-1)\n",
    "\n",
    "    weather_data.temp.fillna((weather_data['lag1_temp']+\n",
    "                                    weather_data['lead1_temp'])/2\n",
    "                                    , inplace=True)\n",
    "    lat_long.sort_values(by = ['location'], ascending=[True], inplace=True)\n",
    "    # haversine(lon1, lat1, lon2, lat2)\n",
    "    unique_location= lat_long['location'].unique()\n",
    "    lat_long_dist = pd.DataFrame([])\n",
    "    for i in xrange(0, len(unique_location)):\n",
    "        for j in xrange(0, len(unique_location)):\n",
    "            if j!=i:\n",
    "                test = lat_long[lat_long['location']==unique_location[i]]\n",
    "                test1 = lat_long[lat_long['location']==unique_location[j]]\n",
    "                lon1 = test['longitude']\n",
    "                lat1 = test['latitude']\n",
    "                lon2 = test1['longitude']\n",
    "                lat2 = test1['latitude']\n",
    "                dist_km = haversine(lon1, lat1, lon2, lat2)\n",
    "                location_1 = unique_location[i]\n",
    "                location_2 = unique_location[j]\n",
    "                dist_mat_latlong = [[location_1,location_2,dist_km]]\n",
    "                lat_long_dist = lat_long_dist.append(dist_mat_latlong)\n",
    "    lat_long_dist= lat_long_dist[np.isfinite(lat_long_dist[2])]  \n",
    "    lat_long_dist.rename(columns={0: 'location', 1:'imp_location',2: 'distance'}, \n",
    "                         inplace=True)\n",
    "    Imputed_temp = pd.DataFrame([])\n",
    "    for i in xrange(0,len(missing_temp_order)): \n",
    "        weather_data_temp = weather_data[weather_data['location'] ==missing_temp_order[i]]\n",
    "        test = lat_long_dist[lat_long_dist['location']==missing_temp_order[i]]\n",
    "        test.sort_values(by = ['distance'], ascending=[True], inplace=True)\n",
    "    #     test_imp = pd.DataFrame([])\n",
    "        impute_temp_order = test['imp_location'].unique()\n",
    "        for j in range (0,len(impute_temp_order)):\n",
    "            weather_data_temp_imp = weather_data[weather_data['location'] ==impute_temp_order[j]]\n",
    "            weather_data_temp_imp = weather_data_temp_imp[['date','hour','temp']]\n",
    "            weather_data_temp_imp.rename(columns = {'temp': 'NN_temp'},inplace = True)\n",
    "            weather_data_temp_test = pd.merge(weather_data_temp,weather_data_temp_imp,\n",
    "                                         how = 'left',\n",
    "                                         on = ['date', 'hour'])\n",
    "        Imputed_temp = Imputed_temp.append(weather_data_temp_test)\n",
    "\n",
    "\n",
    "    Imputed_temp['imp_temp_NN'] = np.where((np.isfinite(Imputed_temp['temp']))\n",
    "                                           ,Imputed_temp['temp'],Imputed_temp['NN_temp'])\n",
    "    unique_location = Imputed_temp['location'].unique()\n",
    "    Imputed_temp_rolling = pd.DataFrame([])\n",
    "    for i in xrange(0,len(unique_location)):\n",
    "        test = Imputed_temp[Imputed_temp['location']==unique_location[i]]\n",
    "        test.sort_values(by = ['hour','date'], ascending = [True,True], inplace = True)\n",
    "        test['rolling_imp_temp_NN']=test['imp_temp_NN'].rolling(window=7,\n",
    "                                                                center=True).median()\n",
    "        Imputed_temp_rolling = Imputed_temp_rolling.append(test)\n",
    "\n",
    "    Imputed_temp_rolling.sort_values(by = ['location','date','hour'],\n",
    "                                    ascending = [True,True,True],\n",
    "                                    inplace = True) \n",
    "    Imputed_temp_rolling['imp_temp_NN'] = np.where((np.isfinite(Imputed_temp_rolling['imp_temp_NN']))\n",
    "                                           ,Imputed_temp_rolling['imp_temp_NN'],\n",
    "                                                   Imputed_temp_rolling['rolling_imp_temp_NN'])\n",
    "    temp_data = Imputed_temp_rolling[['location','date','hour','temp','imp_temp_NN']]\n",
    "    temp_data.sort_values(by = ['location','date','hour'],\n",
    "                         ascending = [True, True, True],\n",
    "                         inplace = True)\n",
    "    temp_data.sort_values(by = ['location','date','hour'], ascending = [True,True,True], inplace = True)\n",
    "    temp_data['lag1_temp'] = temp_data.groupby(['location'])['imp_temp_NN'].shift(1)\n",
    "    temp_data['lead1_temp'] = temp_data.groupby(['location'])['imp_temp_NN'].shift(-1)\n",
    "\n",
    "    temp_data.imp_temp_NN.fillna((temp_data['lag1_temp']+\n",
    "                                    temp_data['lead1_temp'])/2\n",
    "                                    , inplace=True)\n",
    "    temp_data['lag1_temp'] = temp_data.groupby(['location'])['imp_temp_NN'].shift(1)\n",
    "    temp_data['temp_curve'] = temp_data['lag1_temp']/temp_data['imp_temp_NN']\n",
    "    temp_data['lag1_temp_curve'] = temp_data.groupby(['location'])['temp_curve'].shift(1)\n",
    "    temp_data['lead1_temp_curve'] = temp_data.groupby(['location'])['temp_curve'].shift(-1)\n",
    "    temp_data['temp_curve']\n",
    "\n",
    "    unique_location = temp_data['location'].unique()\n",
    "\n",
    "    temp_outlier = pd.DataFrame([])\n",
    "    for i in xrange(0, len(unique_location)):\n",
    "        test = temp_data[temp_data['location']==unique_location[i]]\n",
    "        unique_date = temp_data['date'].unique()\n",
    "        test_outlier = pd.DataFrame([])\n",
    "        for j in xrange(0,len(unique_date)): \n",
    "            test1 = test[test['date']==unique_date[j]]\n",
    "            threshold = 0.05\n",
    "    #         test1['rolling'] = rolling_median(test1['temp_curve'], window=3, center=True).fillna(method='bfill').fillna(method='ffill')\n",
    "            test1['rolling'] = test1['temp_curve'].rolling(window=3,center=True).median()\n",
    "            test1['difference'] = np.abs(test1['temp_curve'] - test1['rolling'])\n",
    "            test1['outlier_idx'] = np.where(((test1['difference'] > threshold) & \n",
    "                                            (np.isfinite(test1['temp']))),1,0)\n",
    "            test_outlier = test_outlier.append(test1)\n",
    "        temp_outlier = temp_outlier.append(test_outlier)\n",
    "\n",
    "    temp_outlier['imp_temp_curve'] = np.where((temp_outlier['outlier_idx'] == 1) \n",
    "                                              ,temp_outlier['rolling'],temp_outlier['temp_curve'])\n",
    "    while True:\n",
    "        count1 = temp_outlier['imp_temp_curve'].count()\n",
    "        temp_outlier['1d_lag_imp_temp_curve']= temp_outlier.groupby(['location'])['imp_temp_curve'].shift(96) \n",
    "        temp_outlier['imp_temp_curve'].fillna(temp_outlier['1d_lag_imp_temp_curve'], inplace=True)\n",
    "        if temp_outlier['imp_temp_curve'].count() - count1==0:\n",
    "            break\n",
    "    unique_location = temp_outlier['location'].unique()\n",
    "    temp_data_test = pd.DataFrame([])\n",
    "    for i in xrange(0,len(unique_location)):\n",
    "        test = temp_outlier[temp_outlier['location']==unique_location[i]]\n",
    "        test['panel_row_id'] = range(1, len(test) + 1 ,1)\n",
    "        temp_data_test=temp_data_test.append(test) \n",
    "    unique_location = temp_data_test['location'].unique()\n",
    "    temp_data_final = pd.DataFrame([]) \n",
    "    for i in xrange(0,len(unique_location)):\n",
    "        test =temp_data_test[temp_data_test['location']==unique_location[i]] \n",
    "        unique_row = temp_data_test['panel_row_id'].unique()\n",
    "        c = np.array(test['imp_temp_curve'])\n",
    "        l = np.array(test['lag1_temp'])\n",
    "        temp_imp_final = np.zeros(len(unique_row))\n",
    "        for j in xrange(1,len(unique_row)):  \n",
    "            if np.isfinite(l[j]):\n",
    "                temp_imp_final[j] = min(l[j]/c[j],44)   \n",
    "            else:\n",
    "                temp_imp_final[j] = min((temp_imp_final[j-1]/c[j-1])/c[j] ,44)\n",
    "        temp_imp_final = pd.DataFrame(temp_imp_final)\n",
    "        temp_imp_final.rename(columns={0: 'temp_imp_final'}, inplace=True)\n",
    "        temp_imp_final['panel_row_id'] = range(1, len(unique_row)+1  ,1)\n",
    "        test = pd.merge(test ,temp_imp_final, how = 'left', on =['panel_row_id'])\n",
    "\n",
    "        temp_data_final = temp_data_final.append(test)\n",
    "        temp_data_final['imputed_temp'] = np.where(((np.isnan(temp_data_final['temp']))\n",
    "                                           |(temp_data_final['outlier_idx']==1)) ,\n",
    "                                            temp_data_final['temp_imp_final'],\n",
    "                                            temp_data_final['temp'])\n",
    "        temp_data_final['temp_imp'] =np.where((np.isnan(temp_data_final['temp'])),\n",
    "                                 temp_data_final['temp_imp_final'],\n",
    "                                            temp_data_final['temp'])\n",
    "\n",
    "    weather_data_temp =temp_data_final[['location','date','hour','temp','temp_imp']] \n",
    "    weather_data_temp.sort_values(by = ['location','date','hour'],\n",
    "                                  ascending = [True,True,True],\n",
    "                                 inplace = True)\n",
    "    return weather_data_temp\n",
    "\n",
    "# test1['rolling'] = test1['temp_curve'].rolling(window=3,center=True).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-20T10:51:02.892635Z",
     "start_time": "2017-05-20T16:21:02.885543+05:30"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if np.max(missing_count['%ge_missing_temp'])>1:\n",
    "    weather_data_temp = impute(weather_data)\n",
    "else:\n",
    "    weather_data_temp = weather_data.copy()\n",
    "    weather_data_temp['temp_imp'] = weather_data_temp['temp']\n",
    "    weather_data_temp =weather_data_temp[['location','date','hour','temp','temp_imp']]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-20T10:51:03.927139Z",
     "start_time": "2017-05-20T16:21:03.913416+05:30"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weather_data_pivot = pd.pivot_table(weather_data_temp, values=['temp_imp'], \n",
    "                                              index=['date','hour'], columns=['location']).reset_index()\n",
    "weather_data_pivot.columns = ['_'.join(col).strip() for col in weather_data_pivot.columns.values]\n",
    "\n",
    "weather_data_pivot.rename(columns={'date_': 'date', 'hour_': 'hour'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-20T10:51:04.874183Z",
     "start_time": "2017-05-20T16:21:04.587069+05:30"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "database flavor mysql is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-95deb15ebb0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mweather_data_pivot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weather_data_pivot_GETCO'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflavor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mysql'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, name, con, flavor, schema, if_exists, index, index_label, chunksize, dtype)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         sql.to_sql(self, name, con, flavor=flavor, schema=schema,\n\u001b[1;32m   1200\u001b[0m                    \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mif_exists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m                    chunksize=chunksize, dtype=dtype)\n\u001b[0m\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, flavor, schema, if_exists, index, index_label, chunksize, dtype)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'{0}' is not valid for if_exists\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mif_exists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m     \u001b[0mpandas_sql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandasSQL_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflavor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflavor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mpandasSQL_builder\u001b[0;34m(con, flavor, schema, meta, is_cursor)\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0mprovided\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \"\"\"\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0m_validate_flavor_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflavor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;31m# When support for DBAPI connections is removed,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36m_validate_flavor_parameter\u001b[0;34m(flavor)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             raise ValueError(\"database flavor {flavor} is not \"\n\u001b[0;32m---> 58\u001b[0;31m                              \"supported\".format(flavor=flavor))\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: database flavor mysql is not supported"
     ]
    }
   ],
   "source": [
    "\n",
    "weather_data_pivot.to_sql(name='weather_data_pivot_GETCO', con=engine,  if_exists='replace', flavor='mysql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#grouped summary with multiple aggregation function \n",
    "\n",
    "weather_summary = weather_data_temp.groupby(['date', 'location'],as_index=False).agg({'temp':{'max':'max',\n",
    "                                                                                              'min':'min',\n",
    "                                                                                              'mean':'mean',\n",
    "                                                                                              'median':'median'}})\n",
    "\n",
    "weather_summary.columns = ['_'.join(col).strip() for col in weather_summary.columns.values]\n",
    "#hourly_weather_summary.to_csv('/Users/Awadhesh/Documents/UPCL_PROJECT/hourly_weather_summary.csv', index=False)\n",
    "weather_summary.rename(columns={'date_': 'date', 'location_' : 'location'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weather_summary['temp_dev'] = weather_summary['temp_max'] - weather_summary['temp_min']\n",
    "weather_summary_pivot = pd.pivot_table(weather_summary, \n",
    "                            values=['temp_median',\n",
    "                                    'temp_mean',\n",
    "                                    'temp_max',\n",
    "                                    'temp_min',\n",
    "                                    'temp_dev'\n",
    "                                    ], \n",
    "                            index=['date'], \n",
    "                            columns=['location']).reset_index()\n",
    "\n",
    "weather_summary_pivot.columns = ['_'.join(col).strip() for col in weather_summary_pivot.columns.values]\n",
    "weather_summary_pivot.rename(columns={'date_': 'date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# weather_summary_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "powercut_table = pd.read_sql_query('select date, block_no, sum(powercut) as powercut from powercut_staging where\\\n",
    "                                   discom = \"GUVNL\" group by date, block_no', engine, index_col = None)\n",
    "powercut_table['date'] = pd.to_datetime(powercut_table['date'])\n",
    "powercut_table.sort_values(by=['date','block_no'], ascending=[True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# powercut_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "load_table = pd.read_sql_query('select date, block_no, constrained_load  \\\n",
    "                             from drawl_staging where discom = \"GUVNL\"', engine, index_col = None)\n",
    "load_table['date'] = pd.to_datetime(load_table['date'])\n",
    "load_table['hour'] = np.ceil(load_table['block_no']/4)\n",
    "load_table['year'] = pd.DatetimeIndex(load_table['date']).year\n",
    "load_table['month'] = pd.DatetimeIndex(load_table['date']).month   # jan = 1, dec = 12\n",
    "load_table['dayofweek'] = pd.DatetimeIndex(load_table['date']).dayofweek # Monday=0, Sunday=6\n",
    "load_table.sort_values(by=['date','block_no'], ascending=[True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load_table.to_csv('/Users/Awadhesh/Documents/LoadForecasting/GETCO_PROJECT/load_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "load_table = pd.merge(load_table, powercut_table,  how = 'left', on = ['date','block_no'])\n",
    "load_table['powercut'].fillna(0, inplace=True)\n",
    "load_table['reported_load'] = load_table['constrained_load']  + load_table['powercut'] \n",
    "load_table.sort_values(by=['date','block_no'], ascending=[True, True], inplace=True)\n",
    "load_table['demand'] = load_table['reported_load'].rolling(window=5,min_periods=1,center=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unique_date = load_table['date'].unique()\n",
    "load_outlier = pd.DataFrame([])\n",
    "for i in xrange(0, len(unique_date)):\n",
    "    test2 = load_table[load_table['date']==unique_date[i]]\n",
    "    threshold = 100\n",
    "    test2['rolling'] = test2['reported_load'].rolling(window=10,center=True).median()\n",
    "    test2['difference'] = np.abs(test2['reported_load'] - test2['rolling'])\n",
    "    test2['outlier_idx'] = np.where((test2['difference'] > threshold),1,0)\n",
    "    load_outlier = load_outlier.append(test2)\n",
    "load_outlier['imp_load'] = np.where((load_outlier['outlier_idx']==1),\n",
    "                                    load_outlier['rolling'],\n",
    "                                    load_outlier['reported_load'])\n",
    "imp_load = load_outlier[['date','block_no','imp_load']]\n",
    "load_table_imp = pd.merge(load_table,imp_load, \n",
    "                          how = 'left',\n",
    "                          on = ['date','block_no'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getEnvelopeModels(aTimeSeries, delta , rejectCloserThan = 0):   \n",
    "    #Prepend the first value of (s) to the interpolating values. This forces the model to use the same starting point for both the upper and lower envelope models.    \n",
    "    u_x = [0,]\n",
    "    u_y = [aTimeSeries[0],]    \n",
    "    lastPeak = 0;\n",
    "    \n",
    "    l_x = [0,]\n",
    "    l_y = [aTimeSeries[0],]\n",
    "    lastTrough = 0;\n",
    "    \n",
    "    #Detect peaks and troughs and mark their location in u_x,u_y,l_x,l_y respectively.    \n",
    "    for k in xrange(1,len(aTimeSeries)- delta):\n",
    "        #Mark peaks        \n",
    "        if (sign(aTimeSeries[k]-aTimeSeries[k-delta]) in (0,1)) and (sign(aTimeSeries[k]-aTimeSeries[k+delta]) in (0,1)) and ((k-lastPeak)>rejectCloserThan):\n",
    "            u_x.append(k)\n",
    "            u_y.append(aTimeSeries[k])    \n",
    "            lastPeak = k;\n",
    "            \n",
    "        #Mark troughs\n",
    "        if (sign(aTimeSeries[k]-aTimeSeries[k-delta]) in (0,-1)) and ((sign(aTimeSeries[k]-aTimeSeries[k+delta])) in (0,-1)) and ((k-lastTrough)>rejectCloserThan):\n",
    "            l_x.append(k)\n",
    "            l_y.append(aTimeSeries[k])\n",
    "            lastTrough = k\n",
    "    \n",
    "    #Append the last value of (s) to the interpolating values. This forces the model to use the same ending point for both the upper and lower envelope models.    \n",
    "    u_x.append(len(aTimeSeries)-1)\n",
    "    u_y.append(aTimeSeries[-1])\n",
    "    \n",
    "    l_x.append(len(aTimeSeries)-1)\n",
    "    l_y.append(aTimeSeries[-1])\n",
    "    \n",
    "    #Fit suitable models to the data. Here cubic splines.    \n",
    "    u_p = interp1d(u_x,u_y, kind = 'cubic',bounds_error = False, fill_value=0.0)\n",
    "    l_p = interp1d(l_x,l_y,kind = 'cubic',bounds_error = False, fill_value=0.0)    \n",
    "    return (u_p,l_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def savitzky_golay(y, window_size, order, deriv=0, rate=1):\n",
    "\n",
    "    import numpy as np\n",
    "    from math import factorial\n",
    "\n",
    "    try:\n",
    "        window_size = np.abs(np.int(window_size))\n",
    "        order = np.abs(np.int(order))\n",
    "    except ValueError, msg:\n",
    "        raise ValueError(\"window_size and order have to be of type int\")\n",
    "    if window_size % 2 != 1 or window_size < 1:\n",
    "        raise TypeError(\"window_size size must be a positive odd number\")\n",
    "    if window_size < order + 2:\n",
    "        raise TypeError(\"window_sizfe is too small for the polynomials order\")\n",
    "    order_range = range(order+1)\n",
    "    half_window = (window_size -1) // 2\n",
    "    # precompute coefficients\n",
    "    b = np.mat([[k**i for i in order_range] for k in range(-half_window, half_window+1)])\n",
    "    m = np.linalg.pinv(b).A[deriv] * rate**deriv * factorial(deriv)\n",
    "    # pad the signal at the extremes with\n",
    "    # values taken from the signal itself\n",
    "    firstvals = y[0] - np.abs( y[1:half_window+1][::-1] - y[0] )\n",
    "    lastvals = y[-1] + np.abs(y[-half_window-1:-1][::-1] - y[-1])\n",
    "    y = np.concatenate((firstvals, y, lastvals))\n",
    "    return np.convolve( m[::-1], y, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import UnivariateSpline, splev, splrep\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def guess(x, y, k, s, w=None):\n",
    "    \"\"\"Do an ordinary spline fit to provide knots\"\"\"\n",
    "    return splrep(x, y, w, k=k, s=s)\n",
    "\n",
    "def err(c, x, y, t, k, w=None):\n",
    "    \"\"\"The error function to minimize\"\"\"\n",
    "    diff = y - splev(x, (t, c, k))\n",
    "    if w is None:\n",
    "        diff = np.einsum('...i,...i', diff, diff)\n",
    "    else:\n",
    "        diff = np.dot(diff*diff, w)\n",
    "    return np.abs(diff)\n",
    "\n",
    "def spline_neumann(x, y, k=3, s=0, w=None):\n",
    "    t, c0, k = guess(x, y, k, s, w=w)\n",
    "    x0 = x[0] # point at which zero slope is required\n",
    "    con = {'type': 'eq',\n",
    "           'fun': lambda c: splev(x0, (t, c, k), der=1),\n",
    "           #'jac': lambda c: splev(x0, (t, c, k), der=2) # doesn't help, dunno why\n",
    "           }\n",
    "    opt = minimize(err, c0, (x, y, t, k, w), constraints=con)\n",
    "    copt = opt.x\n",
    "    return UnivariateSpline._from_tck((t, copt, k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unique_date = load_table_imp['date'].unique()\n",
    "load_envelop = pd.DataFrame([])\n",
    "for j in xrange(1, len(unique_date)):\n",
    "    test = load_table_imp[load_table_imp['date']==unique_date[j]]\n",
    "    s = np.array(test['imp_load'])\n",
    "    P = getEnvelopeModels(s, delta =0, rejectCloserThan = 3)\n",
    "\n",
    "#Evaluate each model over the domain of (s)\n",
    "    q_u = map(P[0],xrange(0,len(s)))\n",
    "    q_l = map(P[1],xrange(0,len(s)))        \n",
    "    test = test[['date','block_no','reported_load', 'demand']].reset_index()\n",
    "    U_envelop = pd.DataFrame(q_u)\n",
    "    U_envelop = U_envelop.rename(columns={0: 'U_envelop'})\n",
    "    L_envelop = pd.DataFrame(q_l)\n",
    "    L_envelop = L_envelop.rename(columns={0: 'L_envelop'})\n",
    "    envelop = pd.concat([test, U_envelop, L_envelop], axis = 1)\n",
    "    load_envelop = load_envelop.append(envelop)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "load_summary = load_table.groupby(['date'],as_index=False).agg({'reported_load':{'std':'std'}})\n",
    "\n",
    "load_summary.columns = ['_'.join(col).strip() for col in load_summary.columns.values]\n",
    "#hourly_weather_summary.to_csv('/Users/Awadhesh/Documents/UPCL_PROJECT/hourly_weather_summary.csv', index=False)\n",
    "load_summary.rename(columns={'date_': 'date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "k=3\n",
    "\n",
    "load_data = load_envelop[['date','block_no', 'reported_load' ,'U_envelop','L_envelop']]\n",
    "load_data.sort_values(by  = ['date', 'block_no'], \n",
    "                      ascending = [True, True],\n",
    "                      inplace = True)\n",
    "spline_Imputed_load = pd.DataFrame([])\n",
    "unique_date = load_data['date'].unique()\n",
    "for i in xrange(0, len(unique_date)):\n",
    "    load_spline = load_data[load_data['date']==unique_date[i]]\n",
    "    std = load_summary[load_summary['date']==unique_date[i]]\n",
    "    std = std['reported_load_std'] \n",
    "    x = np.array(load_spline['block_no'])\n",
    "    y1 = np.array(load_spline['U_envelop'])\n",
    "    y2 = np.array(load_spline['L_envelop'])\n",
    "   \n",
    "    sp0 = UnivariateSpline(x, y1, k=k, s=std*96)\n",
    "    sp01 = UnivariateSpline(x, y2, k=k, s=std*96)\n",
    "    sp1 = spline_neumann(x, y1, k, s=std*96)\n",
    "    sp2 = spline_neumann(x, y2, k, s=std*96)\n",
    "#         sp.set_smoothing_factor(1)\n",
    "    ys1 = sp1(x)\n",
    "    ys1 = pd.DataFrame(ys1)\n",
    "    ys2 = sp2(x)\n",
    "    ys2 = pd.DataFrame(ys2)\n",
    "    ys1.rename(columns={0: 'spline_imp_Uload'}, inplace=True)\n",
    "    ys2.rename(columns={0: 'spline_imp_Lload'}, inplace=True)\n",
    "    ys1['block_no'] = range(1, len(ys1) + 1 ,1)\n",
    "    ys2['block_no'] = range(1, len(ys2) + 1 ,1)\n",
    "    load_spline1 = pd.merge(load_spline, ys1,  how = 'left', on = ['block_no'])\n",
    "    load_spline2 = pd.merge(load_spline1, ys2,  how = 'left', on = ['block_no'])\n",
    "    spline_Imputed_load = spline_Imputed_load.append(load_spline2)\n",
    "spline_Imputed_load['spline_envelop'] = (spline_Imputed_load['spline_imp_Uload'] + \n",
    "                                         spline_Imputed_load['spline_imp_Lload'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spline_Imputed_load = spline_Imputed_load[['date','block_no','reported_load','spline_envelop']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "spline_Imputed_load['endo_demand'] = spline_Imputed_load['spline_envelop']\n",
    "\n",
    "load_table =spline_Imputed_load[['date','block_no', 'reported_load', 'endo_demand']] \n",
    "load_table['hour'] = np.ceil(load_table['block_no']/4)\n",
    "load_table['year'] = pd.DatetimeIndex(load_table['date']).year\n",
    "load_table['month'] = pd.DatetimeIndex(load_table['date']).month   # jan = 1, dec = 12\n",
    "load_table['dayofweek'] = pd.DatetimeIndex(load_table['date']).dayofweek # Monday=0, Sunday=6\n",
    "load_table.sort_values(by = ['date','block_no'], ascending=[True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dayofweek_load_pivot = pd.pivot_table(load_table, values=['endo_demand'], index=['block_no'],\n",
    "                                      columns=['year','month', 'dayofweek'] , aggfunc=[np.mean]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "load_only_table = load_table[['date','block_no','endo_demand','reported_load']]\n",
    "last_date_block = load_only_table[load_only_table['date']== max(load_only_table['date'])]\n",
    "max_block = max(last_date_block['block_no'])\n",
    "columns = [['block_no','endo_demand']]\n",
    "\n",
    "if max_block < 96:\n",
    "    forecast_period0 = pd.DataFrame(columns=columns)\n",
    "    forecast_period0['block_no']=range(max_block+1, 97)\n",
    "    forecast_period0['date'] =max(load_only_table['date']) \n",
    "    forecast_period0 = forecast_period0[['date','block_no','endo_demand']]    \n",
    "else:\n",
    "    forecast_period0 = pd.DataFrame(columns=columns)\n",
    "    forecast_period0['block_no']=range(1, 97)\n",
    "    forecast_period0['date'] =max(load_only_table['date']) + pd.DateOffset(1)\n",
    "    forecast_period0 = forecast_period0[['date','block_no','endo_demand']]\n",
    "\n",
    "    \n",
    "forecast_period = pd.DataFrame([])\n",
    "for j in xrange(1, 8):\n",
    "    period = pd.DataFrame(columns=columns)\n",
    "    period['block_no']=range(1, 97)\n",
    "    period['date'] =max(forecast_period0['date']) + pd.DateOffset(j)\n",
    "    period = period[['date','block_no','endo_demand']]\n",
    "    forecast_period = forecast_period.append(period)\n",
    "    \n",
    "forecast_period_date = pd.concat([forecast_period0, forecast_period] , axis =0)\n",
    "\n",
    "load_only_table = pd.concat([load_only_table, forecast_period_date] , axis =0)\n",
    "\n",
    "non_missing_Load_date = pd.DataFrame(load_only_table.date.unique())\n",
    "non_missing_Load_date.rename(columns={0 : 'date'}, inplace=True)\n",
    "non_missing_Load_date['date'] = pd.to_datetime(non_missing_Load_date['date'])\n",
    "non_missing_Load_date.sort_values(by=['date'], ascending=[False], inplace=True)\n",
    "non_missing_Load_date['date_key'] = range(0, len(non_missing_Load_date))\n",
    "date_key = non_missing_Load_date[['date', 'date_key']]   \n",
    "weather_summary_pivot['date'] = pd.to_datetime(weather_summary_pivot['date'])\n",
    "weather_summary_pivot.sort_values(by=['date'], ascending=[True], inplace=True)\n",
    "weather_summary_nonmissing_load = pd.merge(non_missing_Load_date, weather_summary_pivot, how = 'left', on = ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weather_summary_nonmissing_load = weather_summary_nonmissing_load.sort_values(by = ['date'], \n",
    "                                                                            ascending=[False])                                        \n",
    "daily_weather_mat = weather_summary_nonmissing_load.drop('date', 1)\n",
    "daily_weather_mat = daily_weather_mat.drop('date_key', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weather_summary_nonmissing_load.to_sql(con=engine, name='daily_weather_mat_GETCO', \n",
    "                if_exists='replace', flavor='mysql')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "daily_weather_mat = daily_weather_mat[daily_weather_mat.columns\n",
    "                                               [daily_weather_mat.columns.\n",
    "                                                to_series().str.\n",
    "                                                contains('temp_')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pts = np.array(daily_weather_mat)\n",
    "def distance_matrix_py_neighbour(pts):\n",
    "    n = len(pts)\n",
    "    p = len(pts[0])\n",
    "    #print n, p, m\n",
    "    mat = []\n",
    "    for i in range(n):\n",
    "        if i+365 > n: \n",
    "            t = n\n",
    "        else:\n",
    "            t = i+365\n",
    "        temp = []\n",
    "        for j in range(i,t):\n",
    "            s = 0\n",
    "            for k in range(p):\n",
    "                s = s + (pts[i,k] - pts[j,k])**2\n",
    "            temp.append(s**0.5)\n",
    "        mat.append(temp)   \n",
    "    return mat\n",
    "\n",
    "mat = distance_matrix_py_neighbour(pts)\n",
    "weather_similarity_matrix = mat\n",
    "newmat = []\n",
    "for row in weather_similarity_matrix:\n",
    "    newmat.append([i[0] for i in sorted(enumerate(row), key=lambda x:x[1])])\n",
    "\n",
    "lag_operator = pd.DataFrame(newmat)\n",
    "lag_operator1 = lag_operator[[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lag_operator1.rename(columns={0: 'lag_0', 1: 'lag_1',\n",
    "                   2: 'lag_2', 3: 'lag_3', 4: 'lag_4',\n",
    "                   5: 'lag_5', 6: 'lag_6',7:'lag_7',\n",
    "                   8: 'lag_8', 9: 'lag_9',10:'lag_10',\n",
    "                   11: 'lag_11', 12: 'lag_12',13:'lag_13',\n",
    "                   14: 'lag_14'\n",
    "                  }, inplace=True)\n",
    "\n",
    "lag_operator1['date_key'] = range(0, len(lag_operator1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lag_operator1['lag1'] = lag_operator1['lag_1']+lag_operator1['date_key']\n",
    "lag_operator1['lag2'] = lag_operator1['lag_2']+lag_operator1['date_key']\n",
    "lag_operator1['lag3'] = lag_operator1['lag_3']+lag_operator1['date_key']\n",
    "lag_operator1['lag4'] = lag_operator1['lag_4']+lag_operator1['date_key']\n",
    "lag_operator1['lag5'] = lag_operator1['lag_5']+lag_operator1['date_key']\n",
    "lag_operator1['lag6'] = lag_operator1['lag_6']+lag_operator1['date_key']\n",
    "lag_operator1['lag7'] = lag_operator1['lag_7']+lag_operator1['date_key']\n",
    "lag_operator1['lag8'] = lag_operator1['lag_8']+lag_operator1['date_key']\n",
    "lag_operator1['lag9'] = lag_operator1['lag_9']+lag_operator1['date_key']\n",
    "lag_operator1['lag10'] = lag_operator1['lag_10']+lag_operator1['date_key']\n",
    "lag_operator1['lag11'] = lag_operator1['lag_11']+lag_operator1['date_key']\n",
    "lag_operator1['lag12'] = lag_operator1['lag_12']+lag_operator1['date_key']\n",
    "lag_operator1['lag13'] = lag_operator1['lag_13']+lag_operator1['date_key']\n",
    "lag_operator1['lag14'] = lag_operator1['lag_14']+lag_operator1['date_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lag_operator1 = lag_operator1[['date_key', 'lag1',\n",
    "                               'lag2','lag3','lag4','lag5','lag6','lag7','lag8',\n",
    "                               'lag9','lag10','lag11','lag12','lag13','lag14' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "holiday_event_master = pd.read_sql_query(\"select date, event1 as name from vw_holiday_event_master \\\n",
    "                                            where state = 'UTTARAKHAND'\", engine, index_col = None)\n",
    "holiday_event_master['date'] = pd.to_datetime(holiday_event_master['date'])\n",
    "pre_event_master = holiday_event_master[['date','name']]\n",
    "pre_event_master['date'] =  pre_event_master['date'] - timedelta(days=1)\n",
    "pre_event_master['name'] =  'pre_' + pre_event_master['name'].astype(str)\n",
    "post_event_master = holiday_event_master[['date','name']]\n",
    "post_event_master['date'] =  pd.DatetimeIndex(post_event_master['date']) + timedelta(days=1)\n",
    "post_event_master['name'] =  'post_' + post_event_master['name'].astype(str)\n",
    "event_master = pd.DataFrame([])\n",
    "event_master = event_master.append(holiday_event_master)\n",
    "event_master = event_master.append(pre_event_master)\n",
    "event_master = event_master.append(post_event_master)\n",
    "event_master = event_master.loc[~event_master['date'].duplicated()]\n",
    "event_date = event_master[['date','name']]\n",
    "event_date['holiday_event'] = 1\n",
    "event_calendar = event_date[['date','holiday_event']]\n",
    "date_key_event = pd.merge(date_key,event_calendar, how = 'left', on ='date')\n",
    "date_key_event['dayofweek'] = pd.DatetimeIndex(date_key_event['date']).dayofweek # Monday=0, Sunday=6\n",
    "date_key_event['holiday_event'].fillna(0, inplace = True)\n",
    "date_key_event['weekend_flag'] = np.where((date_key_event['dayofweek']==6),1,0)\n",
    "date_key_event['holiday_flag'] = date_key_event['weekend_flag'] + date_key_event['holiday_event']\n",
    "date_key_event = date_key_event[['date','date_key','holiday_flag']]\n",
    "date_key_event['holiday_flag'] = np.where((date_key_event['holiday_flag'] >=1),1,0)\n",
    "date_key_event = date_key_event[['date_key','holiday_flag']]\n",
    "date_key_event = date_key_event.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lag_operator_weight = pd.merge(lag_operator1,date_key_event, \n",
    "                               how = 'left', left_on = 'lag1', \n",
    "                               right_on ='date_key',indicator=True)\n",
    "lag_operator_weight['weight_lag1'] = 1 - lag_operator_weight['holiday_flag']\n",
    "lag_operator_weight.rename(columns={'date_key_x' : 'date_key'}, inplace=True)\n",
    "lag_operator_weight.drop(['date_key_y', 'holiday_flag'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "lag_operator_weight = pd.merge(lag_operator_weight, date_key_event, \n",
    "                               left_on = 'lag2', right_on ='date_key',how = 'left' )\n",
    "lag_operator_weight['weight_lag2'] = 1 - lag_operator_weight['holiday_flag']\n",
    "lag_operator_weight.rename(columns={'date_key_x' : 'date_key'}, inplace=True)\n",
    "lag_operator_weight.drop(['date_key_y','holiday_flag'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "lag_operator_weight = pd.merge(lag_operator_weight,date_key_event, how = 'left', left_on = 'lag3', right_on ='date_key' )\n",
    "lag_operator_weight['weight_lag3'] = 1 - lag_operator_weight['holiday_flag']\n",
    "lag_operator_weight.rename(columns={'date_key_x' : 'date_key'}, inplace=True)\n",
    "lag_operator_weight.drop(['date_key_y', 'holiday_flag'], axis=1, inplace=True)\n",
    "\n",
    "lag_operator_weight = pd.merge(lag_operator_weight,date_key_event, how = 'left', left_on = 'lag4', right_on ='date_key' )\n",
    "lag_operator_weight['weight_lag4'] = 1 - lag_operator_weight['holiday_flag']\n",
    "lag_operator_weight.rename(columns={'date_key_x' : 'date_key'}, inplace=True)\n",
    "lag_operator_weight.drop(['date_key_y','holiday_flag'], axis=1, inplace=True)\n",
    "\n",
    "lag_operator_weight = pd.merge(lag_operator_weight,date_key_event, how = 'left', left_on = 'lag5', right_on ='date_key' )\n",
    "lag_operator_weight['weight_lag5'] = 1 - lag_operator_weight['holiday_flag']\n",
    "lag_operator_weight.rename(columns={'date_key_x' : 'date_key'}, inplace=True)\n",
    "lag_operator_weight.drop(['date_key_y', 'holiday_flag'], axis=1, inplace=True)\n",
    "\n",
    "lag_operator_weight = pd.merge(lag_operator_weight,date_key_event, how = 'left', left_on = 'lag6', right_on ='date_key' )\n",
    "lag_operator_weight['weight_lag6'] = 1 - lag_operator_weight['holiday_flag']\n",
    "lag_operator_weight.rename(columns={'date_key_x' : 'date_key'}, inplace=True)\n",
    "lag_operator_weight.drop(['date_key_y', 'holiday_flag'], axis=1, inplace=True)\n",
    "\n",
    "lag_operator_weight = pd.merge(lag_operator_weight,date_key_event, how = 'left', left_on = 'lag7', right_on ='date_key' )\n",
    "lag_operator_weight['weight_lag7'] = 1 - lag_operator_weight['holiday_flag']\n",
    "lag_operator_weight.rename(columns={'date_key_x' : 'date_key'}, inplace=True)\n",
    "lag_operator_weight.drop(['date_key_y', 'holiday_flag'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "lag_operator_weight = pd.merge(lag_operator_weight,date_key_event, how = 'left', left_on = 'lag8', right_on ='date_key' )\n",
    "lag_operator_weight['weight_lag8'] = 1 - lag_operator_weight['holiday_flag']\n",
    "lag_operator_weight.rename(columns={'date_key_x' : 'date_key'}, inplace=True)\n",
    "lag_operator_weight.drop(['date_key_y', 'holiday_flag'], axis=1, inplace=True)\n",
    "\n",
    "lag_operator_weight = pd.merge(lag_operator_weight,date_key_event, how = 'left', left_on = 'lag9', right_on ='date_key' )\n",
    "lag_operator_weight['weight_lag9'] = 1 - lag_operator_weight['holiday_flag']\n",
    "lag_operator_weight.rename(columns={'date_key_x' : 'date_key'}, inplace=True)\n",
    "lag_operator_weight.drop(['date_key_y', 'holiday_flag'], axis=1, inplace=True)\n",
    "\n",
    "lag_operator_weight = pd.merge(lag_operator_weight,date_key_event, how = 'left', left_on = 'lag10', right_on ='date_key' )\n",
    "lag_operator_weight['weight_lag10'] = 1 - lag_operator_weight['holiday_flag']\n",
    "lag_operator_weight.rename(columns={'date_key_x' : 'date_key'}, inplace=True)\n",
    "lag_operator_weight.drop(['date_key_y','holiday_flag'], axis=1, inplace=True)\n",
    "\n",
    "lag_operator_weight = pd.merge(lag_operator_weight,date_key_event, how = 'left', left_on = 'lag11', right_on ='date_key' )\n",
    "lag_operator_weight['weight_lag11'] = 1 - lag_operator_weight['holiday_flag']\n",
    "lag_operator_weight.rename(columns={'date_key_x' : 'date_key'}, inplace=True)\n",
    "lag_operator_weight.drop(['date_key_y','holiday_flag'], axis=1, inplace=True)\n",
    "\n",
    "lag_operator_weight = pd.merge(lag_operator_weight,date_key_event, how = 'left', left_on = 'lag12', right_on ='date_key' )\n",
    "lag_operator_weight['weight_lag12'] = 1 - lag_operator_weight['holiday_flag']\n",
    "lag_operator_weight.rename(columns={'date_key_x' : 'date_key'}, inplace=True)\n",
    "lag_operator_weight.drop(['date_key_y','holiday_flag'], axis=1, inplace=True)\n",
    "\n",
    "lag_operator_weight = pd.merge(lag_operator_weight,date_key_event, how = 'left', left_on = 'lag13', right_on ='date_key' )\n",
    "lag_operator_weight['weight_lag13'] = 1 - lag_operator_weight['holiday_flag']\n",
    "lag_operator_weight.rename(columns={'date_key_x' : 'date_key'}, inplace=True)\n",
    "lag_operator_weight.drop(['date_key_y','holiday_flag'], axis=1, inplace=True)\n",
    "\n",
    "lag_operator_weight = pd.merge(lag_operator_weight,date_key_event, how = 'left', left_on = 'lag14', right_on ='date_key' )\n",
    "lag_operator_weight['weight_lag14'] = 1 - lag_operator_weight['holiday_flag']\n",
    "lag_operator_weight.rename(columns={'date_key_x' : 'date_key'}, inplace=True)\n",
    "lag_operator_weight.drop(['date_key_y','holiday_flag'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lag_operator_weight['lag1'] = np.where ((lag_operator_weight['weight_lag1']==0)\n",
    "                                        & (lag_operator_weight['weight_lag1']\n",
    "                                           + lag_operator_weight['weight_lag2']==1),\n",
    "                                            lag_operator_weight['lag2'],\n",
    "                                        np.where ((lag_operator_weight['weight_lag1'] \n",
    "                                                + lag_operator_weight['weight_lag2']==0)\n",
    "                                               & (lag_operator_weight['weight_lag1']\n",
    "                                               + lag_operator_weight['weight_lag2']\n",
    "                                               + lag_operator_weight['weight_lag3']==1),\n",
    "                                                  lag_operator_weight['lag3'],\n",
    "                                        np.where ((lag_operator_weight['weight_lag1'] \n",
    "                                                + lag_operator_weight['weight_lag2']\n",
    "                                                + lag_operator_weight['weight_lag3']==0)\n",
    "                                               & (lag_operator_weight['weight_lag1']\n",
    "                                               + lag_operator_weight['weight_lag2']\n",
    "                                               + lag_operator_weight['weight_lag3']\n",
    "                                               + lag_operator_weight['weight_lag4']==1),\n",
    "                                                  lag_operator_weight['lag4'],\n",
    "                                                  lag_operator_weight['lag1'])))\n",
    "                                                  \n",
    "lag_operator_weight['lag2'] = np.where ((lag_operator_weight['weight_lag2']==0)\n",
    "                                        & (lag_operator_weight['weight_lag2']\n",
    "                                           + lag_operator_weight['weight_lag3']==1),\n",
    "                                            lag_operator_weight['lag3'],\n",
    "                                        np.where ((lag_operator_weight['weight_lag2'] \n",
    "                                                + lag_operator_weight['weight_lag3']==0)\n",
    "                                               & (lag_operator_weight['weight_lag2']\n",
    "                                               + lag_operator_weight['weight_lag3']\n",
    "                                               + lag_operator_weight['weight_lag4']==1),\n",
    "                                                  lag_operator_weight['lag4'],\n",
    "                                        np.where ((lag_operator_weight['weight_lag2'] \n",
    "                                                + lag_operator_weight['weight_lag3']\n",
    "                                                + lag_operator_weight['weight_lag4']==0)\n",
    "                                               & (lag_operator_weight['weight_lag2']\n",
    "                                               + lag_operator_weight['weight_lag3']\n",
    "                                               + lag_operator_weight['weight_lag4']\n",
    "                                               + lag_operator_weight['weight_lag5']==1),\n",
    "                                                  lag_operator_weight['lag5'],\n",
    "                                                  lag_operator_weight['lag2'])))\n",
    "\n",
    "lag_operator_weight['lag3'] = np.where ((lag_operator_weight['weight_lag3']==0)\n",
    "                                        & (lag_operator_weight['weight_lag3']\n",
    "                                           + lag_operator_weight['weight_lag4']==1),\n",
    "                                            lag_operator_weight['lag4'],\n",
    "                                        np.where ((lag_operator_weight['weight_lag3'] \n",
    "                                                + lag_operator_weight['weight_lag4']==0)\n",
    "                                               & (lag_operator_weight['weight_lag3']\n",
    "                                               + lag_operator_weight['weight_lag4']\n",
    "                                               + lag_operator_weight['weight_lag5']==1),\n",
    "                                                  lag_operator_weight['lag5'],\n",
    "                                        np.where ((lag_operator_weight['weight_lag3'] \n",
    "                                                + lag_operator_weight['weight_lag4']\n",
    "                                                + lag_operator_weight['weight_lag5']==0)\n",
    "                                               & (lag_operator_weight['weight_lag3']\n",
    "                                               + lag_operator_weight['weight_lag4']\n",
    "                                               + lag_operator_weight['weight_lag5']\n",
    "                                               + lag_operator_weight['weight_lag6']==1),\n",
    "                                                  lag_operator_weight['lag6'],\n",
    "                                                  lag_operator_weight['lag3'])))\n",
    "\n",
    "lag_operator_weight['lag4'] = np.where ((lag_operator_weight['weight_lag4']==0)\n",
    "                                        & (lag_operator_weight['weight_lag4']\n",
    "                                           + lag_operator_weight['weight_lag5']==1),\n",
    "                                            lag_operator_weight['lag5'],\n",
    "                                        np.where ((lag_operator_weight['weight_lag4'] \n",
    "                                                + lag_operator_weight['weight_lag5']==0)\n",
    "                                               & (lag_operator_weight['weight_lag4']\n",
    "                                               + lag_operator_weight['weight_lag5']\n",
    "                                               + lag_operator_weight['weight_lag6']==1),\n",
    "                                                  lag_operator_weight['lag6'],\n",
    "                                        np.where ((lag_operator_weight['weight_lag4'] \n",
    "                                                + lag_operator_weight['weight_lag5']\n",
    "                                                + lag_operator_weight['weight_lag6']==0)\n",
    "                                               & (lag_operator_weight['weight_lag4']\n",
    "                                               + lag_operator_weight['weight_lag5']\n",
    "                                               + lag_operator_weight['weight_lag6']\n",
    "                                               + lag_operator_weight['weight_lag7']==1),\n",
    "                                                  lag_operator_weight['lag7'],\n",
    "                                                  lag_operator_weight['lag4'])))\n",
    "\n",
    "lag_operator_weight['lag5'] = np.where ((lag_operator_weight['weight_lag5']==0)\n",
    "                                        & (lag_operator_weight['weight_lag5']\n",
    "                                           + lag_operator_weight['weight_lag6']==1),\n",
    "                                            lag_operator_weight['lag6'],\n",
    "                                        np.where ((lag_operator_weight['weight_lag5'] \n",
    "                                                + lag_operator_weight['weight_lag6']==0)\n",
    "                                               & (lag_operator_weight['weight_lag5']\n",
    "                                               + lag_operator_weight['weight_lag6']\n",
    "                                               + lag_operator_weight['weight_lag7']==1),\n",
    "                                                  lag_operator_weight['lag7'],\n",
    "                                        np.where ((lag_operator_weight['weight_lag5'] \n",
    "                                                + lag_operator_weight['weight_lag6']\n",
    "                                                + lag_operator_weight['weight_lag7']==0)\n",
    "                                               & (lag_operator_weight['weight_lag5']\n",
    "                                               + lag_operator_weight['weight_lag6']\n",
    "                                               + lag_operator_weight['weight_lag7']\n",
    "                                               + lag_operator_weight['weight_lag8']==1),\n",
    "                                                  lag_operator_weight['lag8'],\n",
    "                                                  lag_operator_weight['lag5'])))\n",
    "\n",
    "lag_operator_weight['lag6'] = np.where ((lag_operator_weight['weight_lag6']==0)\n",
    "                                        & (lag_operator_weight['weight_lag6']\n",
    "                                           + lag_operator_weight['weight_lag7']==1),\n",
    "                                            lag_operator_weight['lag7'],\n",
    "                                        np.where ((lag_operator_weight['weight_lag6'] \n",
    "                                                + lag_operator_weight['weight_lag7']==0)\n",
    "                                               & (lag_operator_weight['weight_lag6']\n",
    "                                               + lag_operator_weight['weight_lag7']\n",
    "                                               + lag_operator_weight['weight_lag8']==1),\n",
    "                                                  lag_operator_weight['lag8'],\n",
    "                                        np.where ((lag_operator_weight['weight_lag6'] \n",
    "                                                + lag_operator_weight['weight_lag7']\n",
    "                                                + lag_operator_weight['weight_lag8']==0)\n",
    "                                               & (lag_operator_weight['weight_lag6']\n",
    "                                               + lag_operator_weight['weight_lag7']\n",
    "                                               + lag_operator_weight['weight_lag8']\n",
    "                                               + lag_operator_weight['weight_lag9']==1),\n",
    "                                                  lag_operator_weight['lag9'],\n",
    "                                                  lag_operator_weight['lag6'])))\n",
    "\n",
    "lag_operator_weight['lag7'] = np.where ((lag_operator_weight['weight_lag7']==0)\n",
    "                                        & (lag_operator_weight['weight_lag7']\n",
    "                                           + lag_operator_weight['weight_lag8']==1),\n",
    "                                            lag_operator_weight['lag8'],\n",
    "                                        np.where ((lag_operator_weight['weight_lag7'] \n",
    "                                                + lag_operator_weight['weight_lag8']==0)\n",
    "                                               & (lag_operator_weight['weight_lag7']\n",
    "                                               + lag_operator_weight['weight_lag8']\n",
    "                                               + lag_operator_weight['weight_lag9']==1),\n",
    "                                                  lag_operator_weight['lag9'],\n",
    "                                        np.where ((lag_operator_weight['weight_lag7'] \n",
    "                                                + lag_operator_weight['weight_lag8']\n",
    "                                                + lag_operator_weight['weight_lag9']==0)\n",
    "                                               & (lag_operator_weight['weight_lag7']\n",
    "                                               + lag_operator_weight['weight_lag8']\n",
    "                                               + lag_operator_weight['weight_lag9']\n",
    "                                               + lag_operator_weight['weight_lag10']==1),\n",
    "                                                  lag_operator_weight['lag10'],\n",
    "                                                  lag_operator_weight['lag7'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lag_operator1.to_sql(con=engine, name='lag_operator_GETCO', \n",
    "                if_exists='replace', flavor='mysql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "non_missing_Load = pd.merge(load_only_table,non_missing_Load_date, how = 'left', on = ['date'])\n",
    "weather_dist_lag = pd.merge(lag_operator1, non_missing_Load, how='inner', left_on=['lag1'], \n",
    "                             right_on=['date_key'], suffixes=('_x', '_y')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "non_missing_Load = pd.merge(load_only_table,non_missing_Load_date, how = 'left', on = ['date'])\n",
    "weather_dist_lag = pd.merge(lag_operator1, non_missing_Load, how='inner', left_on=['lag1'], \n",
    "                             right_on=['date_key'], suffixes=('_x', '_y')) \n",
    "weather_dist_lag.rename(columns={'date_key_x' : 'date_key', 'endo_demand' : 'Load_NN1'}, inplace=True)\n",
    "weather_dist_lag.drop(['date_key_y', 'date'], axis=1, inplace=True)\n",
    "\n",
    "weather_dist_lag = pd.merge(weather_dist_lag,non_missing_Load, how='left', left_on=['lag2','block_no'], \n",
    "                             right_on=['date_key','block_no'], suffixes=('_x', '_y'))                           \n",
    "weather_dist_lag.rename(columns={'date_key_x' : 'date_key', 'endo_demand' : 'Load_NN2'}, inplace=True)\n",
    "weather_dist_lag.drop(['date_key_y', 'date'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "weather_dist_lag = pd.merge(weather_dist_lag, non_missing_Load, how='left', left_on=['lag3','block_no'], \n",
    "                             right_on=['date_key','block_no'], suffixes=('_x', '_y'))                           \n",
    "weather_dist_lag.rename(columns={'date_key_x' : 'date_key', 'endo_demand' : 'Load_NN3'}, inplace=True)\n",
    "weather_dist_lag.drop(['date_key_y', 'date'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "weather_dist_lag = pd.merge(weather_dist_lag,non_missing_Load, how='left', left_on=['lag4','block_no'], \n",
    "                             right_on=['date_key','block_no'], suffixes=('_x', '_y'))                           \n",
    "weather_dist_lag.rename(columns={'date_key_x' : 'date_key', 'endo_demand' : 'Load_NN4'}, inplace=True)\n",
    "weather_dist_lag.drop(['date_key_y', 'date'], axis=1, inplace=True)\n",
    "\n",
    "weather_dist_lag = pd.merge(weather_dist_lag,non_missing_Load, how='left', left_on=['lag5','block_no'], \n",
    "                             right_on=['date_key','block_no'], suffixes=('_x', '_y'))                           \n",
    "weather_dist_lag.rename(columns={'date_key_x' : 'date_key', 'endo_demand' : 'Load_NN5'}, inplace=True)\n",
    "weather_dist_lag.drop(['date_key_y', 'date'], axis=1, inplace=True)\n",
    "\n",
    "weather_dist_lag = pd.merge(weather_dist_lag,non_missing_Load, how='left', left_on=['lag6','block_no'], \n",
    "                             right_on=['date_key','block_no'], suffixes=('_x', '_y'))                           \n",
    "weather_dist_lag.rename(columns={'date_key_x' : 'date_key', 'endo_demand' : 'Load_NN6'}, inplace=True)\n",
    "weather_dist_lag.drop(['date_key_y', 'date'], axis=1, inplace=True)\n",
    "\n",
    "weather_dist_lag = pd.merge(weather_dist_lag, non_missing_Load, how='left', left_on=['lag7','block_no'], \n",
    "                             right_on=['date_key','block_no'], suffixes=('_x', '_y'))                           \n",
    "weather_dist_lag.rename(columns={'date_key_x' : 'date_key', 'endo_demand' : 'Load_NN7'}, inplace=True)\n",
    "weather_dist_lag.drop(['date_key_y', 'date'], axis=1, inplace=True)\n",
    "\n",
    "weather_dist_lag = pd.merge(weather_dist_lag, non_missing_Load, how='left', left_on=['lag8','block_no'], \n",
    "                             right_on=['date_key','block_no'], suffixes=('_x', '_y'))                           \n",
    "weather_dist_lag.rename(columns={'date_key_x' : 'date_key', 'endo_demand' : 'Load_NN8'}, inplace=True)\n",
    "weather_dist_lag.drop(['date_key_y', 'date'], axis=1, inplace=True)\n",
    "\n",
    "weather_dist_lag = weather_dist_lag.merge(non_missing_Load, how='left', left_on=['lag9','block_no'], \n",
    "                             right_on=['date_key','block_no'], suffixes=('_x', '_y'))                           \n",
    "weather_dist_lag.rename(columns={'date_key_x' : 'date_key', 'endo_demand' : 'Load_NN9'}, inplace=True)\n",
    "weather_dist_lag.drop(['date_key_y', 'date'], axis=1, inplace=True)\n",
    "\n",
    "weather_dist_lag = pd.merge(weather_dist_lag,non_missing_Load, how='left', left_on=['lag10','block_no'], \n",
    "                             right_on=['date_key','block_no'], suffixes=('_x', '_y'))                           \n",
    "weather_dist_lag.rename(columns={'date_key_x' : 'date_key', 'endo_demand' : 'Load_NN10'}, inplace=True)\n",
    "weather_dist_lag.drop(['date_key_y', 'date'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weather_dist_lag = weather_dist_lag[['date_key','block_no', 'Load_NN1', 'Load_NN2', \n",
    "                                     'Load_NN3', 'Load_NN4', 'Load_NN5', \n",
    "                                     'Load_NN6','Load_NN7', 'Load_NN8', \n",
    "                                     'Load_NN9', 'Load_NN10',]]\n",
    "\n",
    "lag_operator_weight = lag_operator_weight[['date_key','weight_lag1', 'weight_lag2', 'weight_lag3', 'weight_lag4',\n",
    "                                          'weight_lag5', 'weight_lag6', 'weight_lag7', 'weight_lag8',\n",
    "                                          'weight_lag9', 'weight_lag10']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weather_dist_lag = pd.merge(weather_dist_lag,lag_operator_weight, how = 'left', on = 'date_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weather_dist_lag = pd.merge(non_missing_Load, weather_dist_lag,  how = 'left' , on = ['date_key','block_no'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weather_dist_lag['hour'] = np.ceil(weather_dist_lag['block_no']/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "date_key = weather_dist_lag[['date','hour','block_no','date_key']]\n",
    "date_key.to_sql(con=engine, name='date_key_GETCO', \n",
    "                if_exists='replace', flavor='mysql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "t=0.4\n",
    "w1=1\n",
    "w2=t\n",
    "w3=t**2\n",
    "w4=t**3\n",
    "w5=t**4\n",
    "w6=t**5\n",
    "w7=t**6\n",
    "\n",
    "weather_dist_lag['endo_pred_sim_day_load']= (weather_dist_lag['Load_NN1']*w1+\n",
    "                                             weather_dist_lag['Load_NN2']*w2+\n",
    "                                             weather_dist_lag['Load_NN3']*w3+\n",
    "                                             weather_dist_lag['Load_NN4']*w4+\n",
    "                                             weather_dist_lag['Load_NN5']*w5+\n",
    "                                             weather_dist_lag['Load_NN6']*w6+\n",
    "                                             weather_dist_lag['Load_NN7']*w7)\\\n",
    "                                             /(w1+w2+w3+w4+w5+w6+w7)\n",
    "    \n",
    "\n",
    "# weather_dist_lag['endo_pred_sim_day_load']=  weather_dist_lag[[\"Load_NN1\", \"Load_NN2\", \n",
    "#                                              \"Load_NN3\", \"Load_NN4\",\"Load_NN5\",\n",
    "#                                              \"Load_NN6\",\"Load_NN7\"]].median(axis=1)\n",
    "\n",
    "weather_dist_lag['similar_day_load']= weather_dist_lag[\"Load_NN1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "similar_day_load = weather_dist_lag[[\"date\",\"block_no\",\"Load_NN1\",\"Load_NN2\",\"Load_NN3\",\n",
    "                                    \"Load_NN4\",\"Load_NN5\",\"Load_NN6\",\"Load_NN7\",\"Load_NN8\",\n",
    "                                    \"Load_NN9\",\"Load_NN10\"]]\n",
    "similar_day_load.to_sql(con=engine, name='similar_day_load_GETCO', \n",
    "                if_exists='replace', flavor='mysql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weather_dist_lag['hour'] = np.ceil(weather_dist_lag['block_no']/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weather_dist_lag['year'] = pd.DatetimeIndex(weather_dist_lag['date']).year\n",
    "weather_dist_lag['month'] = pd.DatetimeIndex(weather_dist_lag['date']).month   # jan = 1, dec = 12\n",
    "weather_dist_lag['week_day']=pd.DatetimeIndex(weather_dist_lag['date']).weekday # mon = 0 , sun = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weather_dist_lag.to_sql(con=engine, name='weather_dist_lag_GETCO', \n",
    "                if_exists='replace', flavor='mysql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# len(weather_dist_lag)/96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count    35808.000000\n",
       " mean         4.171934\n",
       " std          4.935034\n",
       " min          0.000045\n",
       " 25%          1.299641\n",
       " 50%          2.845514\n",
       " 75%          5.298063\n",
       " max         61.002010\n",
       " Name: mape, dtype: float64, count    35808.000000\n",
       " mean         4.231972\n",
       " std          4.933423\n",
       " min          0.000753\n",
       " 25%          1.337827\n",
       " 50%          2.910957\n",
       " 75%          5.410220\n",
       " max         61.288409\n",
       " Name: mape1, dtype: float64)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = weather_dist_lag.copy()\n",
    "test['mape'] = (abs(test['endo_demand'] - test['endo_pred_sim_day_load'])/test['endo_demand'])*100\n",
    "test['mape1'] = (abs(test['reported_load'] - test['endo_pred_sim_day_load'])/test['reported_load'])*100\n",
    "test = test[np.isfinite(test['mape'])]\n",
    "test['mape'].describe() , test['mape1'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
